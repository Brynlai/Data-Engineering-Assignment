{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf935ed-0b46-42f0-9f2c-7aaef6e7e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Lai ZhonPoa\n",
    "\"\"\"\n",
    "# Bryans individual is not limited to BryanIndividual.ipynb\n",
    "from neo4j import GraphDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from UtilsNeo4J import DataBaseHandler\n",
    "from UtilsRedis import Redis_Utilities\n",
    "\n",
    "# Setup Neo4j driver and Redis client 22/12/2024\n",
    "neo4j_uri = \"neo4j+s://75fb82ba.databases.neo4j.io\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"E2znDHtP7x2Hs0B5_BM1tnglu6fTkM5YPTX18DkubIk\" # Replace with your actual password\n",
    "redis_utils = Redis_Utilities()\n",
    "\n",
    "db_handler = DataBaseHandler(neo4j_uri, neo4j_user, neo4j_password, redis_utils)\n",
    "\n",
    "# Get the total number of unique entries in the lexicon\n",
    "total_unique_entries = db_handler.get_total_unique_entries()\n",
    "print(f\"Total number of unique entries: {total_unique_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50fdcae-67b5-4ddb-b611-57aca99c4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Visualization Functions\n",
    "def fetch_synonyms(tx, limit=25):\n",
    "    query = f\"\"\"\n",
    "    MATCH (w:Word)-[:SYNONYM]-(s:Word)\n",
    "    RETURN w.word AS word, collect(DISTINCT s.word) AS synonyms\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    result = tx.run(query)\n",
    "    return result.values()\n",
    "\n",
    "def create_synonym_network(db_handler, limit=25):\n",
    "    with db_handler.neo4j_driver.session() as session:\n",
    "        synonyms = session.read_transaction(fetch_synonyms, limit)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for word, syns in synonyms:\n",
    "        for syn in syns:\n",
    "            G.add_edge(word, syn)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def visualize_network(G):\n",
    "    pos = nx.spring_layout(G, k=0.55)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=5000, font_size=13)\n",
    "    plt.title(\"Synonym Network\")\n",
    "    plt.show()\n",
    "\n",
    "def identify_clusters(G):\n",
    "    clusters = nx.community.greedy_modularity_communities(G)\n",
    "    themes = {i: list(cluster) for i, cluster in enumerate(clusters)}\n",
    "    return themes\n",
    "\n",
    "G = create_synonym_network(db_handler, limit=30)  # Limit to 25 words\n",
    "visualize_network(G)\n",
    "\n",
    "themes = identify_clusters(G)\n",
    "for theme_id, words in themes.items():\n",
    "    print(f\"Theme {theme_id}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa033e1f-afd6-4fed-8a71-1bbb4d0cd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilsRedis import Redis_Utilities\n",
    "import redis\n",
    "word_input = \"sedih\"\n",
    "def get_word_data(word_to_search):\n",
    "    redis_utils = Redis_Utilities()\n",
    "    \n",
    "    sentiment_data = redis_utils.get_sentiment(word_to_search)\n",
    "    \n",
    "    synonyms = db_handler.get_synonyms(word_to_search)\n",
    "    antonyms = db_handler.get_antonyms(word_to_search)\n",
    "    \n",
    "    print(f\"Synonyms for '{word_to_search}': {', '.join(synonyms)}\")\n",
    "    print(f\"Antonyms for '{word_to_search}': {', '.join(antonyms)}\")\n",
    "    print(f\"Sentiment for '{word_to_search}':\", sentiment_data)\n",
    "\n",
    "get_word_data(\"sedih\")\n",
    "get_word_data(\"gembira\")\n",
    "get_word_data(\"a\")\n",
    "get_word_data(\"hasil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f578c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_and_bottom_frequencies(frequencies, top_n=20):\n",
    "    # Convert frequency values to integers for sorting\n",
    "    freq_dict = {word: int(freq) for word, freq in frequencies.items()}\n",
    "    \n",
    "    # Get the top N most used words\n",
    "    most_used = sorted(freq_dict.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
    "    \n",
    "    # Get the top N least used words\n",
    "    least_used = sorted(freq_dict.items(), key=lambda item: item[1])[:top_n]\n",
    "\n",
    "    # Get all words used exactly once\n",
    "    once_used = [word for word, freq in freq_dict.items() if freq == 1]\n",
    "    \n",
    "    return most_used, least_used, once_used\n",
    "\n",
    "# Retrieve frequencies from Redis\n",
    "frequencies = redis_utils.get_all_word_frequencies()\n",
    "\n",
    "# Get the top and bottom frequencies\n",
    "most_used, least_used, once_used = get_top_and_bottom_frequencies(frequencies)\n",
    "\n",
    "# Calculate the maximum length of the words for alignment\n",
    "max_length_most = max(len(word) for word, freq in most_used)\n",
    "max_length_least = max(len(word) for word, freq in least_used)\n",
    "\n",
    "# Print the results in a tidier format\n",
    "print(\"\\nWords used most:\")\n",
    "for word, freq in most_used:\n",
    "    print(f\"{word.ljust(max_length_most)}: {freq}\")\n",
    "\n",
    "print(\"\\nWords used least:\")\n",
    "for word, freq in least_used:\n",
    "    print(f\"{word.ljust(max_length_least)}: {freq}\")\n",
    "\n",
    "print(\"\\nWords used once:\")\n",
    "print(\", \".join(once_used))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
